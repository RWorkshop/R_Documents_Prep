
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\vspace{-1.5cm}
\begin{itemize}
\item The geometric distribution is used for Bernouilli Trials, where the outcome are classified as either failures or successes (either one or the other).
\item The success event is usually the less probable of the two outcomes, although the definitions can be switched if makes the calculation easier.
\end{itemize} 

\end{frame}

%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large

In probability theory, the geometric distribution is either of two discrete probability distributions:
\begin{itemize}
\item The probability distribution of the number of trials needed to get the first success, supported on the set $\{ 1, 2, 3, \ldots\}$, \bigskip
\item The probability distribution of the number of failures before the first success, supported on the set $\{ 0, 1, 2, 3, \ldots\}$
\end{itemize}
\end{frame}

%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\begin{itemize}
\item Which of these one calls "the" geometric distribution is a matter of convention and convenience. 
\item A solution for one type can quickly be surmised from the other.
\item These two different geometric distributions should not be confused with each other. 
\item Often, the name \textbf{shifted geometric distribution} is adopted for the former one (distribution of the number X); 
\item However, to avoid ambiguity, it is good practice to indicate which is intended, by mentioning what type is used explicitly.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\begin{itemize}
\item The following geometric distribution equation is used for modeling the number of trials until the first success. 

\item each with success probability $p$ in each of the independent trials 
 
\item It’s the probability that the $k-$th trial is the first success, after $k-1$ failures in the previous trials, 
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\begin{itemize}
\item If the probability of success on each trial is $p$, then the probability that the $k-$th trial (out of $k$ trials) is the first success is
\[P(X = k) = (1-p)^{k-1}\,p\, \phantom{spa} \mbox{for k} = 1, 2, 3, \ldots \]



\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\begin{itemize}
\item

By contrast, the following form of geometric distribution is used for modeling number of failures until the first success:
\[ P(X=k) = (1 - p)^k\,p\, \phantom{spa} \mbox{for k} = 1, 2, 3, \ldots \]

\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\vspace{-1cm}
\begin{itemize}
\item
In either case, the sequence of probabilities is a geometric sequence.

\item For example, suppose an ordinary die is thrown repeatedly until the first time a "1" appears. 
\item The probability distribution of the number of times it is thrown is supported on the infinite set ${ 1, 2, 3, \ldots }$ and is a geometric distribution with p = 1/6.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\begin{itemize}
\item

The expected value of a geometrically distributed random variable X is 1/p and the variance is $(1 - p)/p^2$:
\[ \mathrm{E}(X) = \frac{1}{p}, \qquad\mathrm{var}(X) = \frac{1-p}{p^2}. \]
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\begin{frame}
\frametitle{The Geometric Distribution}
\Large
\vspace{-1cm}
\begin{itemize}
\item Similarly, the expected value of the geometrically distributed random variable Y (where Y corresponds to the pmf listed in the right column) is (1 - p)/p, 
and its variance is (1 - p)/p2:
\[ \mathrm{E}(Y) = \frac{1-p}{p}, \qquad\mathrm{var}(Y) = \frac{1-p}{p^2}.\]

\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------%
\end{document}


Let µ = (1 - p)/p be the expected value of Y. Then the cumulants \kappa_n of the probability distribution of Y satisfy the recursion
\kappa_{n+1} = \mu(\mu+1) \frac{d\kappa_n}{d\mu}.
Outline of proof: That the expected value is (1 - p)/p can be shown in the following way. Let Y be as above. Then
\begin{align} \mathrm{E}(Y) & {} =\sum_{k=0}^\infty (1-p)^k p\cdot k \\ & {} =p\sum_{k=0}^\infty(1-p)^k k \\ & {} = p\left[\frac{d}{dp}\left(-\sum_{k=0}^\infty (1-p)^k\right)\right](1-p) \\ & {} =-p(1-p)\frac{d}{dp}\frac{1}{p}=\frac{1-p}{p}. \end{align} 
(The interchange of summation and differentiation is justified by the fact that convergent power series converge uniformly on compact subsets of the set of points where they converge.)

\newpage


Geometric distribution

From Wikipedia, the free encyclopedia 

Jump to: navigation, search 


Geometric

Probability mass function
Geometric pmf.svg 
Cumulative distribution function
Geometric cdf.svg 

Parameters
0< p \leq 1 success probability (real) 0< p \leq 1 success probability (real) 

Support
k \in \{1,2,3,\dots\}\! k \in \{0,1,2,3,\dots\}\! 

Probability mass function (pmf)
(1 - p)^{k-1}\,p\! (1 - p)^{k}\,p\! 

Cumulative distribution function (CDF)
1-(1 - p)^k\! 1-(1 - p)^{k+1}\! 

Mean
\frac{1}{p}\! \frac{1-p}{p}\! 

Median
\left\lceil \frac{-1}{\log_2(1-p)} \right\rceil\! (not unique if -1/\log_2(1-p) is an integer) \left\lceil \frac{-1}{\log_2(1-p)} \right\rceil\! - 1 (not unique if -1/\log_2(1-p) is an integer) 

Mode
1 0 

Variance
\frac{1-p}{p^2}\! \frac{1-p}{p^2}\! 

Skewness
\frac{2-p}{\sqrt{1-p}}\! \frac{2-p}{\sqrt{1-p}}\! 

Excess kurtosis
6+\frac{p^2}{1-p}\! 6+\frac{p^2}{1-p}\! 

Entropy
\tfrac{-(1-p)\log_2 (1-p) - p \log_2 p}{p}\! \tfrac{-(1-p)\log_2 (1-p) - p \log_2 p}{p}\! 

Moment-generating function (mgf)
\frac{pe^t}{1-(1-p) e^t}\!,
for t<-\ln(1-p)\! \frac{p}{1-(1-p)e^t}\! 

Characteristic function
\frac{pe^{it}}{1-(1-p)\,e^{it}}\! \frac{p}{1-(1-p)\,e^{it}}\! 