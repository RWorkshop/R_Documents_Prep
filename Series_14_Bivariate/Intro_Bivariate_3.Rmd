
### Correlation
Recall that correlation describes the strength of a relationship between two numeric variables, and that the Pearson product-moment correlation coefficient is a measure of the strength of the linear relationship between two variables.

It is referred to as Pearson's correlation or simply as the correlation coefficient. If the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables.

The symbol for Pearson's correlation is "$\rho$" when it is measured in the population and <tt>r</tt> when it is measured in a sample.

As we will be dealing almost exclusively with samples, we will use <tt>r</tt> to to represent Pearson's correlation unless otherwise noted.

Pearson's r can range from -1 to 1. An r of -1 indicates a perfect negative linear relationship between variables, an <tt>r</tt> of 0 indicates no linear relationship between variables, and an <tt>r</tt> of 1 indicates a perfect positive relationship between variables.

Importantly it is assumed that the relationship in question is supposed to be linear. Some variables will in fact have a non-linear relationship (more on that latet)

The relevant R command is <tt>cor()</tt>.



<pre><code>
cor(immer$Y1,immer$Y2)

cor(iris[,1],iris[,3])
</code></pre>

The strength of the relation is represented in a numeric value known at the correlation coefficient. This coefficient can take a value between -1 and 1. 

Additionally there are no units.

Getting a correlation coefficient is generally only half the story; you will want to know if the relationship is significant. There is a more complex command called <tt>cor.test()</tt>. This command additionally provides a hypothesis test for the correlation estimate.


<pre><code>
cor.test(immer$Y1,immer$Y2)

cor.test(iris[,1],iris[,3])
</code></pre>



* [Ho] : The correlation coefficient for the population of values is zero. (i.e. No linear relationship.)
* [Ha]: The coefficient is not zero. (Linear relationship exists.)
	


A confidence interval for the coefficient is provided for in the <tt>R</tt> output. If the interval includes 0 then we fail to reject the null hypothesis.

Simple linear regression is used to describe the relationship between two variables ‘x’ and ‘y’.

For example, you may want to describe the relationship between age and blood pressure or the relationship between scores in a midterm exam and scores in the final exam, etc.


* 	$x$ is the independent (i.e. predictor) variable.
* 	$y$ is the dependent (i.e. response) variable.
That is to say $x$ is said to cause or influence $y$.

Necessarily both x and y should be of equal length. One of the first steps in a regression analysis is to determine if any kind of relationship exists between $x$ and $y$.

A scatterplot can created and can initially be used to get an idea about the nature of the relationship between the variables, e.g. if the relationship is linear, curvilinear, or no relationship exists.

To make a simple scatter-plot, we simply use the <tt>plot()</tt> command. The independent variable (the variable to go along the x-axis) is always specified first.




<pre><code>
X=c(5.98, 8.80, 6.89, 8.49, 8.48, 7.47, 7.97,5.94, 7.32, 6.64, 6.94, 3.51)

Y=c(5.56, 7.80, 6.13, 8.15, 7.95, 7.87, 8.03, 5.67, 7.11, 6.65, 7.02, 3.88)

plot(X,Y)
cor(X,Y)
</code></pre>

In this case here, we can see from the scatter-plot that there is a linear relationship between x and y.
Simple linear regression is only useful when there is evidence of a linear relationship. In other cases, such as quadratic relationships, other types of regression may be more appropriate.

### Linear Regression Model}

A linear relationship can be defined by the simple linear regression model
\[y = \beta_0 + \beta_1x + \epsilon\]

The intercept $\beta_0$ describes the point at which the line intersects the y axis.
The slope $\beta_1$ describes the change in ‘y’ for every unit increase in the predictor variable $x$.

From the data set, we determine the regression coefficients, i.e. estimates for slope and intercept. (N.B. There are variations on this notation in textbooks).

	*  $b_0$ : the intercept estimate.
* 	$b_1$ : the slope estimate.

Therefore the fitted model can be expressed as

\[ \hat{y} = b_0 + b_1x \]
Recall $\hat{y}$  denotes the predicted value for y, given some value x.

### Fitting a Model with \texttt{R</tt>

The  <tt>R</tt> command   <tt>lm()</tt> is used to fit linear models. Firstly the response variable $y$  is specified, then the predictor variable $x$.

The tilde sign is used to denote the dependent relationship (i.e. y depends on x). The regression coefficients are then determined.


<pre><code>
lm(Y~X) # y depends on X
</code></pre>


The output will include the formula, and two coefficient terms

*  The intercept estimate is recorded under $(Intercept)$
*  The slope estimate is recorded under the name of the predictor variable (here : $X$ ).
	
	
<pre><code>
Call:
lm(formula = Y ~ X)

Coefficients:
(Intercept)            X
     0.7812       0.8581
</code></pre>

A more detailed data output (i.e. more than just the coefficients) is generated in the form of a data object, using the \textbf{\texttt{summary()</tt> command.

We can give a name to the model (e.g. $FIT1$), and view all of the results of the calculation, including the regression coefficients, hypothesis test results and information on the residuals (i.e. the differences between the estimated ‘y’ values and the observed ‘y’ values).

In common with all data structures we can use the \textbf{\texttt{names()</tt> function and ‘$\$$’ to access components.


<pre><code>
FIT1 = lm(Y~X)
summary(FIT1)
names(FIT1)
names(summary(FIT1))
FIT1$coefficients
class(FIT1)
</code></pre>

\newpage
### Confidence Interval for Regression Estimate}
To compute the confidence intervals for both estimates, we use the <tt>confint()</tt> command, specifying the name of the fitted model.

<pre><code>
C=c(0,2,4,6,8,10,12)
F=c(2.1,5.0,9.0,12.6,17.3,21.0,24.7)
Fit1=lm(F~C)
coef(Fit1)
# (Intercept)        Conc
     1.517857    1.930357

confint(Fit1)
#               2.5 %   97.5 %
# (Intercept) 0.75970 2.276014
# Conc        1.82522 2.035495
</code></pre>


### The Coefficient of Determination}
The coefficient of determination $R^2$ is the proportion of variability in a data set that is accounted for by the linear model.

Equivalently $R^2$ provides a measure of how well future outcomes are likely to be predicted by the model.

(For simple linear regression, it canbe computed by squaring the correlation coefficient.)


<pre><code>
summary(fit1)$r.squared
</code></pre>

