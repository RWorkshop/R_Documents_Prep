---
title: Importing Data into R
author: DragonflyStats.github.io
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### 6.2 Import
Import the data file you downloaded. Refer to Chapter 5 for importing data into R.

<pre><code>
import <- read_csv("Data Files/tidyverse_example.csv")
</code></pre>

* It is always a good idea to get to know your dataframe before you start messing with it. 
* What are the column names? What kind of values are stored in each column? How many observations are there? How many Subjects? How many Trials? etc.

What are the column names? use colnames() for a quick glance at the column names

colnames(import)
##  [1] "Subject"              "TrialProc"            "Trial"                "Condition"            "RT"                   "ACC"                 
##  [7] "Response"             "TargetArrowDirection" "SessionDate"          "SessionTime"
To take a quick look at the first few rows of a dataframe use head().

<pre><code>
head(import)
## # A tibble: 6 x 10
##   Subject TrialProc Trial Condition      RT   ACC Response TargetArrowDirection SessionDate SessionTime
##     <dbl> <chr>     <dbl> <chr>       <dbl> <dbl> <chr>    <chr>                <chr>       <time>     
## 1   14000 practice      1 incongruent  1086     1 left     left                 08-30-2017  10:30:25   
## 2   14000 practice      2 incongruent   863     1 left     left                 08-30-2017  10:30:25   
## 3   14000 practice      3 congruent     488     1 right    right                08-30-2017  10:30:25   
## 4   14000 practice      4 incongruent   588     1 right    right                08-30-2017  10:30:25   
## 5   14000 practice      5 congruent     581     1 right    right                08-30-2017  10:30:25   
## 6   14000 practice      6 incongruent   544     1 right    right                08-30-2017  10:30:25
</code></pre>
This gives you a good idea of what column names you will be working with and what kind of values they contain.

To evaluate what are all the unique values in a column you can use unique(). You can also use this in combination with length() to evaluate how many unique values are in a column.

unique(import$Condition)
## [1] "incongruent" "congruent"
unique(import$Trial)
##   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
##  [36]  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70
##  [71]  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105
## [106] 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140
## [141] 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175
## [176] 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192
max(import$Trial)
## [1] 192
length(unique(import$Subject))
## [1] 410
unique(import$TrialProc)
## [1] "practice" "real"
unique(import$ACC)
## [1] 1 0
All these functions we just used from colnames() to unique() were to temporarily evaluate our data. They are not required to perform the actual data analysis. Therefore, I usually just type these in the console. A general rule of thumb is that if it is not required to be saved in your Script file then just type it in the console.

Okay let’s take a look at how to use the dplyr functions to score this data.


--------------------------



{Importing Data}
=======================


New packages for reading data into R — fast
Hadley Wickham and the RStudio team have created some new packages for R, which will be very useful for anyone who needs to read data into R (that is, everyone). The readr package provides functions for reading text data into R, and the readxl package provides functions for reading Excel spreadsheet data into R. Both are much faster than the functions you're probably using now.



The readr package provides several functions for reading tabular text data into R. This is a task normally accomplished with the read.table family of functions in R, and readr provides a number of replacement functions that provide additional functionality and are much faster.


%- http://blog.revolutionanalytics.com/2015/04/new-packages-for-reading-data-into-r-fast.html





First, there's read_table which provides a near-replacement for read.table. Here's a comparison of using both functions on a file with 4 million rows of data (which I created by stacking copies of this file):
\begin{framed}
\begin{verbatim}
dat <- read_table("biggerfile.txt",
  col_names=c("DAY","MONTH","YEAR","TEMP"))

dat2 <- read.table("biggerfile.txt", 
  col.names=c("DAY","MONTH","YEAR","TEMP"))
\end{verbatim}
\end{framed}
The commands look quite similar, but while read.table took just over 30 seconds to complete, readr's read_table accomplished the same task in less than a second. The trick is that read_table treats the data as a fixed-format file, and uses C++ to process the data quickly. (One small caveat is that read.table supports arbitrary amounts of whitespace between columns, while read_table requires the columns be lined up exactly. In practice, this isn't much of a restriction.)

%==

{Importing Data}


Base R has a function for reading fixed-width data too, and here readr really shines:
\begin{framed}
\begin{verbatim}
dat <- read_fwf("biggerfile.txt", 
  fwf_widths(c(3,15,16,12),
    col_names=c("DAY","MONTH","YEAR","TEMP")))

dat2 <- read.fwf("biggerfile.txt", c(3,15,16,12),
  col.names=c("DAY","MONTH","YEAR","TEMP"))

\end{verbatim}
\end{framed}

----------------------------


While readr's read_fwf again accomplished the task in about a second, the standard read.fwf took over 3 minutes — almost 200 times as long.

Other functions in the package include read_csv (and a European-friendly variant read_csv2) for comma-separated data, read_tsv for tab-separated data, and read_lines for line-by-line file extraction (great for complicated post-processing). The package also makes it much easier to read columns of dates in various formats, and sensibly always handles text data as strings (no more strings.as.factors=FALSE). 


----------------------------


For data in Excel format, there's also the new readxl package. This package provides function to read Excel worksheets in both .xls and .xlsx formats. I haven't benchmarked the read_excel function myself, but like the readr functions it's based on a C++ library so should be quite snappy. And best of all, it has no external dependencies, so you can use it to read Excel data on just about any platform — there's no requirement that Excel itself be installed.

The readr package is on CRAN now, and readxl can be installed from GiHub. If you try them yourself, let us know how it goes in the comments.

\end{framed}

%==





I’m pleased to announced that readr is now available on CRAN. Readr makes it easy to read many types of tabular data:


%==

{Importing Data}



Delimited files with``read_delim()``, ``read_csv()``, ``read_tsv()``, and ``read_csv2()``.
Fixed width files with ``read_fwf()``, and ``read_table()``.
Web log files with ``read_log()``.
You can install it by running:

install.packages("readr")
Compared to the equivalent base functions, readr functions are around 10x faster. They’re also easier to use because they’re more consistent, they produce data frames that are easier to use (no more stringsAsFactors = FALSE!), they have a more flexible column specification, and any parsing problems are recorded in a data frame. Each of these features is described in more detail below.


%==

{Importing Data}


Input
All readr functions work the same way. There are four important arguments:


\item
file gives the file to read; 
* a url or local path. A local path can point to a a zipped, bzipped, xzipped, or gzipped file. it’ll be automatically 
uncompressed in memory before reading. You can also pass in a connection or a raw vector.

For small examples, you can also supply literal data: if file contains a new line, then the data will be read directly from the string. Thanks to data.table for this great idea!


%==

{Importing Data}


\begin{framed}
\begin{verbatim}

library(readr)
``read_csv("x,y\n1,2\n3,4")
#>   x y
#> 1 1 2
#> 2 3 4
\end{verbatim}
\end{framed}

``col_names: describes the column names (equivalent to header in base R). It has three possible values:
TRUE will use the the first row of data as column names.
FALSE will number the columns sequentially.
A character vector to use as column names.

``col_types: overrides the default column types (equivalent to colClasses in base R). More on that below.
progress: By default, readr will display a progress bar if the estimated loading time is greater than 5 seconds. Use progress = FALSE to suppress the progress indicator.



%==

{Importing Data}

\noindent \textbf{Output}\\
The output has been designed to make your life easier:


* Characters are never automatically converted to factors (i.e. no more stringsAsFactors = FALSE!).
* Column names are left as is, not munged into valid R identifiers (i.e. there is no check.names = TRUE). Use backticks to refer to variables with unusual names, e.g. df$`Income ($000)`.
* The output has class c("tbl_df", "tbl", "data.frame") so if you also use dplyr you’ll get an enhanced print method (i.e. you’ll see just the first ten rows, not the first 10,000!).
Row names are never set.



%==

{Importing Data}


Column types
Readr heuristically inspects the first 100 rows to guess the type of each columns. This is not perfect, but it’s fast and it’s a reasonable start. Readr can automatically detect these column types:

``col_logical()`` [l], contains only T, F, TRUE or FALSE.
``col_integer()`` [i], integers.
``col_double()`` [d], doubles.
``col_euro_double()`` [e], “Euro” doubles that use , as the decimal separator.
``col_date()`` [D]: Y-m-d dates.
``col_datetime()`` [T]: ISO8601 date times
``col_character()`` [c], everything else.


%==

{Importing Data}


You can manually specify other column types:

``col_skip()`` [_], don’t import this column.
``col_date(format) and ``col_datetime(format, tz), dates or date times parsed with given format string. Dates and times are rather complex, so they’re described in more detail in the next section.
``col_numeric()`` [n], a sloppy numeric parser that ignores everything apart from 0-9, - and . (this is useful for parsing currency data).
``col_factor(levels, ordered), parse a fixed set of known values into a (optionally ordered) factor.


%==

{Importing Data}


There are two ways to override the default choices with the ``col_types argument:

Use a compact string: "dc__d". Each letter corresponds to a column so this specification means: read first column as double, second as character, skip the next two and read the last column as a double. (There’s no way to use this form with column types that need parameters.)
With a (named) list of col objects:
``read_csv("iris.csv", ``col_types = list(
  Sepal.Length = ``col_double()``,
  Sepal.Width = ``col_double()``,
  Petal.Length = ``col_double()``,
  Petal.Width = ``col_double()``,
  Species = ``col_factor(c("setosa", "versicolor", "virginica"))
))


%==

{Importing Data}



Any omitted columns will be parsed automatically, so the previous call is equivalent to:

``read_csv("iris.csv", ``col_types = list(
  Species = ``col_factor(c("setosa", "versicolor", "virginica"))
)


%==

{Importing Data}


DATES AND TIMES
One of the most helpful features of readr is its ability to import dates and date times. It can automatically recognise the following formats:

Dates in year-month-day form: 2001-10-20 or 2010/15/10 (or any non-numeric separator). It can’t automatically recongise dates in m/d/y or d/m/y format because they’re ambiguous: is 02/01/2015 the 2nd of January or the 1st of February?
Date times as ISO8601 form: e.g. 2001-02-03 04:05:06.07 -0800, 20010203 040506, 20010203 etc. I don’t support every possible variant yet, so please let me know if it doesn’t work for your data (more details in ?parse_datetime).
If your dates are in another format, don’t despair. You can use ``col_date()`` and ``col_datetime()`` to explicit specify a format string. Readr implements it’s own strptime()`` equivalent which supports the following format strings:


%==

{Importing Data}



Year: \%Y (4 digits). \%y (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999.
Month: \%m (2 digits), \%b (abbreviated name in current locale), \%B (full name in current locale).
Day: \%d (2 digits), \%e (optional leading space)
Hour: \%H
Minutes: \%M
Seconds: \%S (integer seconds), \%OS (partial seconds)
Time zone: \%Z (as name, e.g. America/Chicago), \%z (as offset from UTC, e.g. +0800)
Non-digits: \%. skips one non-digit charcater, \%* skips any number of non-digit characters.
Shortcuts: \%D = \%m/\%d/\%y, \%F = \%Y-\%m-\%d, \%R = \%H:\%M, \%T = \%H:\%M:\%S, \%x = \%y/\%m/\%d.
To practice parsing date times with out having to load the file each time, you can use parse_datetime()`` and parse_date()``:

parse_date("2015-10-10")
#> [1] "2015-10-10"
parse_datetime("2015-10-10 15:14")
#> [1] "2015-10-10 15:14:00 UTC"

parse_date("02/01/2015", "%m/%d/%Y")
#> [1] "2015-02-01"
parse_date("02/01/2015", "%d/%m/%Y")
#> [1] "2015-01-02"
Problems
If there are any problems parsing the file, the ``read_ function will throw a warning telling you how many problems there are. You can then use the problems()`` function to access a data frame that gives information about each problem:

csv <- "x,y
1,a
b,2
"

df <- ``read_csv(csv, ``col_types = "ii")
#> Warning: 2 problems parsing literal data. See problems(...) for more
#> details.
problems(df)
#>   row col   expected actual
#> 1   1   2 an integer      a
#> 2   2   1 an integer      b
df
#>    x  y
#> 1  1 NA
#> 2 NA  2


%==

{Importing Data}


Helper functions
Readr also provides a handful of other useful functions:

``read_lines()`` works the same way as readLines()``, but is a lot faster.
``read_file()`` reads a complete file into a string.
``type_convert()`` attempts to coerce all character columns to their appropriate type. 
This is useful if you need to do some manual munging (e.g. with regular expressions) to turn strings into numbers. It uses the same rules as the ``read_* functions.
``write_csv()`` writes a data frame out to a csv file. It’s quite a bit faster than ``write.csv()`` and it never writes row.names. 
It also escapes " embedded in strings in a way that ``read_csv()`` can read.


%==

{Importing Data}

\noindent \textbf{Development}\\

* Readr is still under very active development. If you have problems loading a dataset, please try the development version, and if that doesn’t work, file an issue.



