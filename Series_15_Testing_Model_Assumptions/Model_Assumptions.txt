---
title: "Assessing Model Assumptions"
subtitle: "Time Series Analysis with R"
author: DragonflyStats.github.io
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
```
## Assessing Model Assumptions}
### Residuals}  The difference between the predicted value (based on the regression equation) and the actual, observed value. In simple linear regression models, the matter of whether or not residuals are normally distributed often arises.

Additionally the expected value of the residuals should be zero.

We have seen previously two methodologies for determining whether or not a data set is normally distributed;

 *  	Shapiro-Wilk tests (or Anderson-Darling test)
*  	QQ plots


We will explore this more in a forthcoming example.
### Influence Analysis}


\subsubsection{Outlier} In linear regression, an outlier is an observation with large residual.  In other words, it is an observation whose dependent-variable value is unusual given its values on the predictor variables.  An outlier may indicate a sample peculiarity or may indicate a data entry error or other problem.
\subsubsection{Leverage}  An observation with an extreme value on a predictor variable is a point with high leverage.  Leverage is a measure of how far an independent variable deviates from its mean.  These leverage points can have an effect on the estimate of regression coefficients.

\subsubsection{Influence} An observation is said to be influential if removing the observation substantially changes the estimate of coefficients.  Influence can be thought of as the product of leverage and outlierness.
### Example}
A new hotel is built 15 miles from the location of a prominent annual sporting event. A study of the number of enquiries received by a random sample of 9 established hotels in the area showed that the number of enquiries and the distance in miles between the hotel and event. Here the independent variable is distance (x) and the dependent variable is number of enquiries.

Lets looks at the residuals, and assess whether they are normally distributed.

\begin{framed}
\begin{verbatim}
#enquiries
y=c(35,61,74,92,113,159,188,217,328)
 	
#distance from hotel
x=c(28,20,17,12,16,8,2,3,1)	
#

#fit the linear model	
fit2=lm(y~x)					
resid(fit2)
res.fit=resid(fit2)

# test the residuals for normality.
# Normal if p.value is high.
shapiro.test(res.fit)	
	
qqnorm(res.fit)	#QQ plot
qqline(res.fit)	#Add Trendline


#Do all your analyses agree?

\end{verbatim}
\end{framed}
Let’s look at the scatterplot of x and y (<tt>plot(x,y)}}).  Does the first covariate seem to be an outlier, given that a linear model is assumed?
Lets omit the first element of both data sets and run the analysis again.

\begin{framed}
\begin{verbatim}
fit2=lm(y[-1]~x[-1])
resid(fit2)
res.fit2=resid(fit2)

shapiro.test(res.fit2)			

#test the residuals for normality. Normal if p.value is high.
qqnorm(res.fit2);  qqline(res.fit2)			

# compare the coefficients of both models.
coef(fit1)
coef(fit2)

\end{verbatim}
\end{framed}
Does the covariate in question have high leverage or high influence?


Remark: Arguably it is a case that this problem is not best described by a simple linear regression model, and that a non-linear model would be more suitable.

### Diagnostic Plots}
\textbf{\emph{Homoscedascity }}(constant variance) is one of the assumptions required in a regression analysis in order to make valid statistical inferences about population relationships.

Homoscedasticity requires that the variance of the residuals are constant for all fitted values, indicated by a uniform scatter or dispersion of data points about the trend line (i.e. ``The Zero Line").

From the above plot, we can conclude that the constant variance assumption is valid. We can see that the mean value of the residuals is zero.
\begin{framed}
\begin{verbatim}
plot(fit1)
#Four Diagnostic Plots are printed to screen sequentially.
\end{verbatim}
\end{framed}
\newpage
## Multiple Linear Regression}
In your future studies, you will come across multiple linear regression (MLR). This is a linear model uses multiple independent variables to explain a single dependent variable.

The implementation is very similar to simple linear regression (SLR). All that is required is to specify the additional independent variables.

\begin{framed}
\begin{verbatim}
Fit.slr =lm(y~x)  	# SLR: y explained by predictor x
Fit.mlr=lm(y~x+z)  # MLR: y explained by predictors x and z
\end{verbatim}
\end{framed}

For this case, a  linear relationship can be defined by the regression model  \[y =\beta_0 + \beta+1x + \beta_2z + \epsilon\].

Again, we determine the regression coefficients, i.e. estimates for slopes and intercept. (N.B. There are variations on this notation).


* 	$b_0$ : the intercept estimate.
* 	$b_1$  : the slope estimate for X
* 	$b_2$  : the slope estimate for z


In many project datasets it is possible to implement a MLR model. For the moment, we will just look at slope and intercept estimates, their p-values and the coefficient of determination.

Let try this out using the \textbf{\textit{iris}} data set. (This is not be a valid statistical analysis in practice. However we are focussing on the mechanics, so we shall proceed nonetheless).
\begin{framed}
\begin{verbatim}
lm(Sepal.Length ~ Sepal.Width + Petal.Width)
\end{verbatim}
\end{framed}



### Model Selection}
There are many important methodologies for determining which combination of predictor variables bests describes a response variable. You will meet this in future modules.
We will use two simple ones for this module only.

*  Adjusted R–squared value
 
