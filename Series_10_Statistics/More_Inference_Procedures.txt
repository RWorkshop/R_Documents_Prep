

\section{Part 2 More inference procedures}
\subsection{Formal tests for Normality}

\begin{itemize}
\item ad.test for outliers Anderson Darling test
\item shapiro Wilk test for normality
\item ks.test for distributions (Kolmogorov Smirnov test)
\item grubbs test for outliers
\item chi square test
\end{itemize}

useful for categorical data. Consider a two by two contingency table.


apropos test to find other tests available in R. Chi-squared Test of Independence

Two random variables x and y are called independent if the probability distribution of one variable is not affected by the presence of another.

Assume fij is the observed frequency count of events belonging to both i-th category of xand j-th category of y. Also assume eij to be the corresponding expected count if x and yare independent. The null hypothesis of the independence assumption is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level α.



Example

 Graphical procedures for assessing normality

qqplot

histograms


\subsection{Quantile-Quantile Plots}

Description

qqnorm is a generic function the default method of which produces a normal QQ plot of the values in y. qqline adds a line to a normal quantile-quantile plot which passes through the first and third quartiles.

qqplot produces a QQ plot of two datasets.

Graphical parameters may be given as arguments to qqnorm, qqplot and qqline.

%-------------------------------------------------------------------------------%


Part 3 linear model

scatterplots


\subsection{Scatter Plot:}

 A scatter plot of two variables shows the values of one variable on the Y axis and the values of the other variable on the X axis. Scatter plots are well suited for revealing the relationship between two variables. The scatter plot shown in Figure 4 illustrates data from one of Galileo's classic experiments in which he observed the distance traveled balls traveled after being dropped on a incline as a function of their release height.



A standard practice when performing a bivariate analysis is to construct a scatter plot. This is easily implementable using the R command plot.

Note that the predictor variable precedes the response variable.

More complex scatterplots, with better visual appeal, can be constructed. We will lok at this more later on in the semester.


The Pearson product-moment correlation coefficient is a measure of the strength of the linear relationship between two variables. It is referred to as Pearson's correlation or simply as the correlation coefficient. If the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables.

The symbol for Pearson's correlation is "ρ" when it is measured in the population and "r" when it is measured in a sample. Because we will be dealing almost exclusively with samples, we will use r to to represent Pearson's correlation unless otherwise noted.

Pearson's r can range from -1 to 1. An r of -1 indicates a perfect negative linear relationship between variables, an r of 0 indicates no linear relationship between variables, and an r of 1 indicates a perfect positive relationship between variables. 


%% - http://onlinestatbook.com/chapter4/intro.html

%=====================================================%
\subsection{Simple Linear Regression}


The intercept is the estimated value of the response variable when the predictor variable is set to zero. In many cases it would make sense that there would be a non zero value in this instance.

The slope is the rate at which response variable changes as the predictor cbanfes. A negative slope insicates a negative linear relationship.

%=====================================================%
\subsection{Fitting a Model}

 To fit an ordinary linear model with fertility change as the response and setting and effort as predictors, try

\begin{framed}
\begin{verbatim}
 > lmfit = lm( change ~ setting + effort )
\end{verbatim}
\end{framed}

 Note first that lm is a function, and we assign the result to an object that we choose to call lmfit (for linear model fit). This stores the results of the fit for later examination


\subsection{Correlation }

\begin{itemize}
\item Correlation describes the strength of a relationship between two numeric variables.
\item The strength of the relation is represented in a numeric value known at the correlation coefficient. This coefficient can take a value between -1 and1. Additionally there are no units.

\item A value close to 1 indicates strong linear relation. A value close to Zero indicates no linear relationship at all.importantly it is assumed that the relationship is linear. Some variables will have a non linear relationship.

\item The relevant R command is cor(). There is also a more complex command called cor.test. This command additionally provides a hypothesis test for the estimate.
\begin{itemize}
\item[Ho:] The correlation coefficient for the population of values is zero. No linear relationship.
\item[Ha:] The coefficient is not zero. Linear relationship exists.
\end{itemize}
\item A confidence interval for the coefficient is provided for in the R output. If the interval includes 0 then we fail to reject the null hypothesis.
\end{itemize}
\subsection{Anscombe Quartet}
Anscombe quartet is a famous data set used to demonstrate why a full set of statistical analyses should be used in conjunction, rather than rely on one analysis. This data set comprises four set of data.
%====================================================================================%

Correlation and Significance tests

Getting a correlation coefficient is generally only half the story; you will want to know if the relationship is significant. The cor() function in R can be extended to provide the significance testing required. The function is cor.test()


To run a correlation test we type:
\begin{framed}
\begin{veratim}
> cor.test(var1, var2, method = "method")
\end{verbatim}
\end{framed}

The default method is "pearson" so you may omit this if that is what you want. If you type "kendall" or "spearman" then you will get the appropriate significance test.

As usual with R it is a good idea to assign a variable name to your result in case you want to perfom additional operations.

Simple Linear Regression

Correlation Test

correlation

pearsons rho



 multiple linear regression


linear regression whete there is more than one predictor variable.

very simple to implement in R.


coefficient of determination. 

for sime lunear regression this is equivalent to the correlation squared


also rank correlation coefficient.

Spearman correlation




explanatory variables

predictor variables

slope intercept regression  equation

plitting both variables against each other

histograms 


underlying assumptions normal distribution of residual terms

%======================================================================================%
influence

outliers

 non linear regression

 polynomial regression

