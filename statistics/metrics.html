<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="RWorkshop.GitHub.io : Website">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>RWorkshop.GitHub.io</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
          <h1 id="project_title">RWorkshop.GitHub.io</h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
       
<!-- ############################################# -->
{Review of SLR}
<ul>
<li> Used function <tt>lm()</tt> to fit model.
<li> Determined regression coefficients.
<li> Residuals and fitted values.
<li> Used summary() to determine inference values for the regression coefficients.
<li> Correlation, scatterplots etc.
</ul>



<!-- ############################################# -->
{Data}
<ul>

<li>
The data give scores for the taste of a cheese (Taste) from 30 different formulations which caused variation
in the concentration in the cheese of acetic acid (Acetic), hydrogen sulphide (H2S)
and lactic acid (Lactic).

<li> data in MLR.r

<li> For SLR, using Acetic as only explanatory variable (for simplicity).
</ul>





\section{SLR Diagnostics}

<ul>
<li> Once the model has been fitted, we must check the residuals.
<li> The residuals should be independent and normally distributed with
mean of 0 .
<li> Use a histogram of the residuals
<li> Can also use a Q-Q plot. ( i.e. qqnorm())
<li> The residuals should must have constant variance.
<li> Use a plot of fitted values vs residuals.
</ul>


<!-- ############################################# -->

{SLR Diagnostics}
<pre><code>

>model =lm(Taste ~ Acetic)
>par(mfrow=c(2,2))
>       #Histogram
>hist(resid(model))
>       #QQ plot
>qqnorm(resid(model))
>plot(fitted(model), resid(model))
>par(mfrow=c(1,1))	#reset
</code></pre>

The residuals seem to be normally distributed, judging by the histogram and QQ plot.
There seems to be consistent variance too in the residual plot, but a possible ``funnell effect" .



<!-- ############################################# -->
 {Multiple regression}
Multiple regression is carried out when there is more than one
explanatory variable.
The multiple linear regression model has the form:\\
$y = \beta_{0} + \beta_{1}x1 + \beta_{2}x2 ....+ \beta_{p}Xp + \epsilon$\\
Where the beta values are the regression coefficients and the 'x's are the predictor variables.\\
The number of predictor variables is 'p'.



<!-- ############################################# -->
{Cheese Data}
<ul>

<li> We wish to model the dependence of the taste score
on the concentrations of those three constituents, using the n = 30 observations.

<li> We also wish to assess if one of the constituents has no effect at all.
<li> We can build various candidate models and choose the best.

</ul>

<!-- ############################################# -->

{MLR}
A number of models were fitted.
<pre><code>

>model1=lm(Taste~ Acetic + H2S)
>model2 =lm(Taste~ Acetic + Lactic)
>model3 =lm(Taste~ Lactic + H2S)
>model4 =lm(Taste~ Acetic + Lactic + H2S)
</code></pre>




<!-- ############################################# -->
 {Coefficient of determination}
<ul>
<li> One can measure how well the model succeeds in explaining the variation in the
response by the Coefficient of Determination $R^2$, (which is defined by the ratio of the
Sum of squares for the slopes to the Total sum of squares in ANOVA)

<li> $R^2 = \frac{p S_{\beta}^2}{(n-1)S_{y}^2}$

<li> $R^2$ found using the summary() function also.

<li> The higher the value of $R^2$ the better.

<li> $R^2$ is usually thought of as the proportion of the variation in the response variable
explained by the regression. <li> Often, one would look for $R^2$ over 60\% before thinking
that a model was useful.
</ul>

<!-- ############################################# -->
 {Multiple Regression}
<ul>
<li> $R^2$  is the square of the correlation coefficient between the values of the
response variable and the fitted values from the model.

<li> There is also an F-test for the Null Hypothesis that the population analogue of R2 is 0 against
the alternative that it is greater than 0. This is a test of whether there is any point in
fitting the regression at all.
<li> A first check is to look at the scatter plots of
the response variable Taste against each of the explanatory variables.
</ul>

<!-- ############################################# -->-----
{R squared}
<pre><code>
> summary(model1)$r.squared
[1] 0.5821773
> summary(model2)$r.squared
[1] 0.5202762
> summary(model3)$r.squared
[1] 0.6517024
> summary(model4)$r.squared
[1] 0.6517747
</code></pre>
Model 4 has the highest value for $R^2$.

<!-- ############################################# -->
 {Multiple Regression}
<ul>
<li> If the number of explanatory variables is increased, then $R^2$ always decreases.
 So if one wants to choose how many explanatory variables to include in the model,
one cant depend entirely on $R^2$, because that would always suggest putting in every
explanatory variable available.

<li> One should look also at Students t values for the estimated
slopes.

<li> The Adjusted Coefficient of Determination, which makes
an attempt to correct for the number of explanatory variables used (i.e. penalizes using too many)

<li> Some explanatory variables may give the same information as each other. So all are not required.


</ul>

<!-- ############################################# -->

{Adjusted R squared}
<pre><code>
> summary(model1)$adj.r.squared
[1] 0.5512274
> summary(model2)$adj.r.squared
[1] 0.4847411
> summary(model3)$adj.r.squared
[1] 0.6259025
> summary(model4)$adj.r.squared
[1] 0.6115948
</code></pre>
Model 3 has the highest value for Adjusted $R^2$.

