\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{fancyhdr}
%\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MS4024} \rhead{Mr. Kevin O'Brien}
\chead{Numerical Computation}
%\input{tcilatex}

\begin{document}

\tableofcontents
\newpage

\section{Inference Procedures}
Key Points:
\begin{itemize}
\item The two main types of inference procedures are \textbf{Hypothesis Tests} and \textbf{Confidence Intervals}. You are expected to be familiar with both.

\item There are two ways of conducting a hypothesis test. One method is to compute the test statistic, and compare to the critical values.

\item The second method is to compute the probability value  (i.e. p-value), and compare it to the significance level. Nearly all computer programs use the p-value approach. In this course we will focus on the p-value approach.
\end{itemize}

\subsection{Significance Level}

In hypothesis testing, the significance level is the criterion used for rejecting the null hypothesis. The significance level is used in hypothesis testing as follows: First, the difference between the results of the experiment and the null hypothesis is determined. Then, assuming the null hypothesis is true, the probability of a difference that large or larger is computed . Finally, this probability is compared to the significance level. If the probability is less than or equal to the significance level, then the null hypothesis is rejected and the outcome is said to be statistically significant.

Traditionally, experimenters have used either the 0.05 level (sometimes called the $5\%$ level) or the 0.01 level ($1\%$ level), although the choice of levels is largely subjective. The lower the significance level, the more the data must diverge from the null hypothesis to be significant.
 Therefore, the 0.01 level is more conservative than the 0.05 level. The Greek letter alpha ($\alpha$) is sometimes used to indicate the significance level

\subsection{The probability value }

The probability value (sometimes called the $p-value$) is the probability of obtaining a statistic as different from or more different from the parameter specified in the null hypothesis as the statistic obtained in the experiment.

\subsubsection{The precise meaning of the p-value}

There is often confusion about the precise meaning of the probability computed in a significance test. The convention in hypothesis testing is that the null hypothesis (Ho) is assumed to be true.

The difference between the statistic computed in the sample and the parameter specified by the null hypothesis is computed and the probability of obtaining a difference this large or large is calculated. This probability value is the probability of obtaining data as extreme or more extreme than the current data (assuming the null hypothesis is true).

It is not the probability of the null hypothesis itself. Thus, if the probability value is 0.005, this does not mean that the probability that the null hypothesis is either true or false is 0.005. It means that the probability of obtaining data as different or more different from the null hypothesis as those obtained in the experiment is 0.005.

The inferential step to conclude that the null hypothesis is false goes as follows: The data (or data more extreme) are very unlikely given that the null hypothesis is true.
This means that:

(1) a very unlikely event occurred or

(2) the null hypothesis is false.

The inference usually made is that the null hypothesis is false.  (Importantly it doesn't prove the null hypothesis to be false)

\subsection{Using p-values to reject the null hypothesis}
According to one view of hypothesis testing, the significance level should be specified before any statistical calculations are performed. Then, when the p-value is computed from a significance test, it is compared with the significance level.

The null hypothesis is rejected if p-value is at or below the significance level; it is not rejected if p-value is above the significance level. The degree to which p ends up being above or below the significance level does not matter.The null hypothesis either is or is not rejected at the previously stated significance level.

Thus, if an experimenter originally stated that he or she was using the α = 0.05 significance level and p-value was subsequently calculated to be 0.042, then the person would reject the null hypothesis at the 0.05 level. If p-value had been 0.0001 instead of 0.042 then the null hypothesis would still be rejected at the 0.05 significance level.

The experimenter would not have any basis to be more confident that the null hypothesis was false with a p-value of 0.0001 than with a p-value of 0.041. Similarly, if the p had been 0.051 then the experimenter would fail to reject the null hypothesis

The experimenter would have no more basis to doubt the validity of the null hypothesis than if p-value had been 0.482. The conclusion would be that the null hypothesis could not be rejected at the 0.05 level.

In short, this approach is to specify the significance level in advance and use p-value only to determine whether or not the null hypothesis can be rejected at the stated significance level.

Many statisticians and researchers find this approach to hypothesis testing not only too rigid, but basically illogical. It is very reasonable to  have more confidence that the null hypothesis is false with a p-value of 0.0001 then with a p-value of 0.042?

The less likely the obtained results (or more extreme results) under the null hypothesis, the more confident one should be that the null hypothesis is false.

The null hypothesis should not be rejected once and for all. The possibility that it was falsely rejected is always present, and, all else being equal, the lower the p-value, the lower this possibility.

According to this view, research reports should not contain the p-value, only whether or not the values were significant (at or below the significance level).

However it is much more reasonable to just report the p-values. That way each reader can make up his or her mind about just how convinced they are that the null hypothesis is false.

\subsubsection{Guidelines for Data Project}
For this module, as a rule of thumb, we will use the threshold of 0.01 for rejecting the null hypothesis. If the p-value is less than 0.01 we reject the null hypothesis. If it is greater than 0.05, we fail to reject the null hypothesis. If between the two, consider it to be a `grey area'. (i.e. suggest that more data is needed).

If the p-value is greater than 0.1 we would never reject the null hypothesis.
\begin{itemize} 
\item Greater than 0.05 - Fail to reject Ho
\item Less than 0.01 - Reject Ho
\item Between 0.01 and 0.05 - advise that it is close to both conclusions.
\end{itemize}


%\subsection{Probability Values}
%The probability of getting a values as extreme or more as some statistic, such as sum or mean, is known as a p-value. When performing statistical calculations using computer software they are the most commonly used item for making statistical decisions.
%
%In this last instance, we would usually fail to reject the null hypothesis.

Many \texttt{R} outputs will give a group of asterisks beside the data to help the user in interpreting the data, depending on how significant the result is.

\begin{verbatim}
p-value  < 0.0001  	***
p-value  < 0.001	**
p-value  < 0.01	*
p-value  < 0.1
\end{verbatim}



\subsection{Sample Size}
For Student's $t$ distribution, statistical tables such (e.g. Murdoch Barnes and State Examinations Commission tables) only tabulate quantiles with degrees of freedom of less than 30. This restraint has given rise to the convention that a sample of size greater than 30 is a `large sample' and in this case the standard normal distribution should be used.

However there is a disparity between the $Z$ value and the correct $t$ value. For a sample size of 61 (i.e. degrees of freedom =60), the $97.5\%$ t-quantiles of Student's t distribution is 2.003, and not 1.96.

However, statistical software is free from this restraint. The correct distribution will be automatically used. The Student's $t$ distribution will be used in all appropriate cases. As the sample size increases the Student $t$ distribution converges with the standard normal distribution.

\subsection{Commonly Used Inference Procedures}
\begin{itemize}
\item	   Hypothesis test for the mean of a single sample
\item	   Hypothesis test for the mean of two independent samples
\item	   Hypothesis test for the proportion of a single group
\item	   Hypothesis test for the proportions of two independent samples
\end{itemize}

\section{Testing The Assumption of Normality}
For example, a fundamental assumption of linear models (i.e. regression models) is that the residuals (differences between observed and predicted value) are normally distributed with mean zero.


The null hypothesis of both the `Anderson-Darling' and `Shapiro-Wilk' tests is that the population is normally distributed, and the alternative hypothesis is that the data is not normally distributed.

For both tests, the null and alternative hypothesis are :\\
\qquad $H_0 : $ The data set is normally distributed.\\
\qquad $H_1 : $ The data set is \textbf{not} normally distributed.\\

\subsection{Anderson-Darling Test}
To implement the Anderson-Darling Test for Normality, one must first install the \textbf{\emph{nortest}} package.

\begin{framed}
\begin{verbatim}
library(nortest)
#Generate 100 normally distributed random numbers
NormDat = rnorm(100)
ad.test(NormDat)
\end{verbatim}
\end{framed}
\subsection{Shapiro-Wilk Test}
The Shapiro-Wilk test is directly implementable, without loading any additional packages.

\begin{framed}
\begin{verbatim}
#Generate 100 normally distributed random numbers

NormDat = rnorm(100)

shapiro.test(NormDat)
\end{verbatim}
\end{framed}
Sample output, using the randomly generated \texttt{NormDat} data set, is as follows:
\begin{verbatim}
> shapiro.test(NormDat)

        Shapiro-Wilk normality test

data:  NormDat
W = 0.9864, p-value = 0.4003
\end{verbatim}
Here, the p-value is well above the 0.05 threshold. Hence we \textbf{fail to reject} the null hypothesis, and may proceed to treat the \texttt{NormDat} data set as normally distributed.
\subsection{Graphical Procedures for Assessing Normality}
There are two useful graphical methods for determining whether a data set was normally distributed. The first is the histogram, which we have seen previously. If the histogram is reasonably bell-shaped, then the data can be assumed to be normally distributed. The relevant R command is \texttt{\textbf{hist()}}.


The second is the \textbf{\emph{quantile-quantile plot}} (or QQ-plot).
For assessing normality, we implement a qq-plot  using the \texttt{\textbf{qqnorm()}} function.

Additionally the command \texttt{\textbf{qqline()}} function adds a trendline to a normal quantile-quantile plot. If the data is normally distributed, then the points on the plot follow the trendline.

\begin{framed}
\begin{verbatim}
#Generate 100 normally distributed random numbers

NormDat = rnorm(100)

qqnorm(NormDat)
qqline(NormDat)
\end{verbatim}
\end{framed}

% Section 8 Testing Normality
\subsection{Transforming the Data}

Sometimes when we get non-normal data, we can change the scale of our data i.e. transform it to get a normal distribution. One transformation that often works for positively skewed data is the natural logarithm (ln) transformation.

In such a case, we work with the natural logarithms of the data set, rather than the data itself.
\subsection{Outliers}
Another reason that the data may not be normally distributed is the presence of an outlier. We shall look at formal tests for outliers (such as the Grubb's test) next week. Recall that boxplots can be used to detect potential outliers.

\section{Single Sample Inference Procedures}

While analyzing a (single) sample of data values, we will often want to answer several questions:
\begin{itemize}
\item	What is the mean value?  (i.e. of 100 roles of a die)
\item	Is the mean value significantly different from some pre-supposed value?
(i.e. Hypothesis testing ; is the observed mean reasonably close to our expected value)
\item	What is the level of uncertainty associated with our estimate of the mean value? (i.e. Confidence interval for the estimates)
\end{itemize}
We refer to the methods used to answer these questions as \textbf{\emph{inference procedures}}.





%---------------------------------------------------- %
\newpage
\section{Bivariate data}
\subsection{What is Bivariate data?}

A dataset with two variables contains what is called bivariate data. For example, the heights and weights of people (i.e. for the purposes of determining the extent to which taller people weigh more)

Common bivariate statistical analyses include
\begin{itemize}
\item Correlation
\item Simple Linear Regression
\end{itemize}

\subsection{Scatter Plot} A scatter plot of two variables shows the values of one variable on the Y axis and the values of the other variable on the X axis. Scatter plots are well suited for revealing the relationship between two variables.

Scatterplots can be implemented in \texttt{R} using the command \texttt{\textbf{plot()}}

Exercise: Let us construct scatter-plots for the Immer and Iris data sets.

\begin{framed}
\begin{verbatim}
plot(immer$Y1,immer$Y2)

plot(iris[,1],iris[,3])
\end{verbatim}
\end{framed}
More complex scatterplots, with better visual aesthetics, can be constructed. We will look at this more later on in the semester.




We can give a name to the model (e.g. $FIT1$), and view all of the results of the calculation, including the regression coefficients, hypothesis test results and information on the residuals (i.e. the differences between the estimated ‘y’ values and the observed ‘y’ values).

In common with all data structures we can use the \textbf{\texttt{names()}} function and ‘$\$$’ to access components.

\begin{framed}
\begin{verbatim}
FIT1 = lm(Y~X)
summary(FIT1)
names(FIT1)
names(summary(FIT1))
FIT1$coefficients
class(FIT1)
\end{verbatim}
\end{framed}
\newpage
\subsection{Confidence Interval for Regression Estimate}
To compute the confidence intervals for both estimates, we use the \texttt\textbf{{confint()}} command, specifying the name of the fitted model.
\begin{framed}
\begin{verbatim}
C=c(0,2,4,6,8,10,12)
F=c(2.1,5.0,9.0,12.6,17.3,21.0,24.7)
Fit1=lm(F~C)
coef(Fit1)
# (Intercept)        Conc
     1.517857    1.930357

confint(Fit1)
#               2.5 %   97.5 %
# (Intercept) 0.75970 2.276014
# Conc        1.82522 2.035495
\end{verbatim}
\end{framed}

\subsection{The Coefficient of Determination}
The coefficient of determination $R^2$ is the proportion of variability in a data set that is accounted for by the linear model.

Equivalently $R^2$ provides a measure of how well future outcomes are likely to be predicted by the model.

(For simple linear regression, it canbe computed by squaring the correlation coefficient.)

\begin{framed}
\begin{verbatim}
summary(fit1)$r.squared
\end{verbatim}
\end{framed}
\newpage


The estimates (i.e regression coefficients) may be determined directly using the 
coef() command.

.
A fundamental assumption of linear models is that the
Residuals are normally distributed with mean zero. Investigate this using the RESID command.

The Anderson Darling test is the conventional inference test for assessing whether a data set is normally distibuted. It requires liading the normtest pagkage into the R environment. A nother inference procedure fir testing normality is the Shapiro Wilk test. (We will use this one fir this module, but use Anderson Darling in future).

shapiro.test(x)

The null hypothesis is that the data set is normally distribted. We reject this null hypothesis if the pvalue is too small.

The summary function provides a comprehensive overview of all inference estimates for a model fit, including pvalues for all predictor variables.

The summary output is constructed as a list, and as such, components can be aaccessed using the dollar sign.(Find the names of the component using the names command)

coef(fit1)

returns the intercept and slope estmates for the model.

The adjusted coefficient of determination is computed to account for the presence of more than one predictor variable.
The law of parsimony, the simplest model that adequately explains the outcomes is the best.





\subsection{Model Selection}
There are many important methodologies for determining which combination of predictor variables bests describes a response variable. You will meet this in future modules.
We will use two simple ones for this module only.
\begin{itemize}
\item Adjusted R–squared value
\item The Akaike Information Criterion (AIC)
\end{itemize}


The adjusted R-square value is found on the summary output for a fitted model. It is called \textbf{\emph{adjusted}} because it takes into account the number of predictor variables being used. The law of parsimony states the simplest model that adequately explains the outcomes is the best. The candidate model with the higher adjusted R squared is considered preferable.

The AIC is a model selection metric often used in statistics.It is computed using the R command
\texttt{\textbf{AIC()}}.The candidate model with the smallest AIC value is considered preferable.

\begin{framed}
\begin{verbatim}
fitA = lm(Sepal.Length ~ Sepal.Width + Petal.Width)
fitB = lm(Sepal.Length ~ Sepal.Width + Petal.Length)

summary(fitA)$adj.r.squared
summary(fitB)$adj.r.squared

AIC(fitA)
AIC(fitB)
\end{verbatim}
\end{framed}

%----------------------------------------------------%
\end{document} 
