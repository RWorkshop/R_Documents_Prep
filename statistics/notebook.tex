

Part 1 inference procedures

Two-Tailed Test of Population Mean with Known Variance

Population Mean Between Two Matched Samples

Test for variances

The paired t test

T-test

Part 2 More inference procedures

Formal tests for Normality

chi square test

Quantile-Quantile Plots

Description

Part 3 linear model

scatterplots

Simple Linear Regression

Fitting a Model

Correlation

Correlation and Significance tests

Simple Linear Regression

multiple linear regression

coefficient of determination.




Part 1 inference procedures

t test

null hypothesis alternative hypothesis

pairedt test 
%==================================%
\subsection{ \texttt{prop.test()} }


test for proportions. Speciify the number if successes.  The number of trials, and the expected value of the probability under the null hypothesis.

%============================================================================================================%
\subsection{Two-Tailed Test of Population Mean with Known Variance}

The null hypothesis of the two-tailed test of the population mean can be expressed as follows:



where μ0 is a hypothesized value of the true population mean μ.

Let us define the test statistic z in terms of the sample mean, the sample size and thepopulation standard deviation σ :



Then the null hypothesis of the two-tailed test is to be rejected if z ≤−zα∕2 or z ≥ zα∕2 , wherezα∕2 is the 100(1 − α∕2) percentile of the standard normal distribution.

\subsection{Problem}

Suppose the mean weight of King Penguins found in an Antarctic colony last year was 15.4 kg. In a sample of 35 penguins same time this year in the same colony, the mean penguin weight is 14.6 kg. Assume the population standard deviation is 2.5 kg. At .05 significance level, can we reject the null hypothesis that the mean penguin weight does not differ from last year?

\subsection{Solution}

The null hypothesis is that μ = 15.4. We begin with computing the test statistic.

\begin{framed}
\begin{verbatim}
> xbar = 14.6            # sample mean 

> mu0 = 15.4             # hypothesized value 

> sigma = 2.5            # population standard deviation 

> n = 35                 # sample size 

> z = (xbar−mu0)/(sigma/sqrt(n)) 

> z                      # test statistic 

[1] −1.8931
\end{verbatim}
\end{framed}
We then compute the critical values at .05 significance level.

\begin{framed}
\begin{verbatim}
> alpha = .05 

> z.half.alpha = qnorm(1−alpha/2) 

> c(−z.half.alpha, z.half.alpha) 

[1] −1.9600  1.9600
\end{verbatim}
\end{framed}
Answer

The test statistic -1.8931 lies between the critical values -1.9600 and 1.9600. Hence, at .05 significance level, we do not reject the null hypothesis that the mean penguin weight does not differ from last year.

\subsection{Population Mean Between Two Matched Samples}

Two data samples are matched if they come from repeated observations of the same subject. Here, we assume that the data populations follow the normal distribution. Using the paired t-test, we can obtain an interval estimate of the difference of the population means.


Example

In the built-in data set named immer, the barley yield in years 1931 and 1932 of the same field are recorded. The yield data are presented in the data frame columns Y1 and Y2.

\begin{framed}
\begin{verbatim}
> library(MASS)         # load the MASS package 

> head(immer) 

 Loc Var    Y1    Y2 

1  UF   M  81.0  80.7 

2  UF   S 105.4  82.3 

   .....

\end{verbatim}
\end{framed}
Problem

Assuming that the data in immer follows the normal distribution, find the 95% confidence interval estimate of the difference between the mean barley yields between years 1931 and 1932.

Solution

We apply the t.test function to compute the difference in means of the matched samples. As it is a paired test, we set the "paired" argument as TRUE.

> t.test(immer$Y1, immer$Y2, paired=TRUE) 



          Paired t-test 



data:  immer$Y1 and immer$Y2 

t = 3.324, df = 29, p-value = 0.002413 

alternative hypothesis: true difference in means is not equal to 0 

95 percent confidence interval: 

 6.122 25.705 

sample estimates: 

mean of the differences 

                15.913

Answer

Between years 1931 and 1932 in the data set immer, the 95\% confidence interval of the difference in means of the barley yields is the interval between 6.122 and 25.705.

\subsection{Test for variances}


var.test test that two data sets have equal variance.


test that mean is some specified value. 


e.g. Mean =3.50

e.g. 

mean is not equal to 3.50 


The test for equality of variances is important, as many other inference procedures rely on the assumption that the two data sets under consideration have equal variance. Two Sample T test.

\subsection{The paired t test }

The paired t test is another important inference procedure. It takes two sets of paired measurements and tests for a difference. It is called a paired test because each value in one data set  correspond to a value in the other data set. Necessarily there must be equal numbers of elements in both sets.


P values are the probability of a test statistic under the null hypothesis. They are commonly misinterpreted as the probability that a hypothesis is true. Including court cases with tragic consequences.

%============================================================================================%



\subsection{T-test}

The t-test is used to determine statistical differences between two samples. There is also a version that can be used as a paired test i.e. when you have measurements collected as matched pairs.


Attach your data set so that the individual variables are read into memory.

To perform a t-test you type:

\begin{framed}
\begin{verbatim}
> t.test(var1, var2)

Welch Two Sample t-test

data: x1 and x2

t = 4.0369, df = 22.343, p-value = 0.0005376

alternative hypothesis: true difference in means is not equal to 0

95 percent confidence interval:

2.238967 6.961033

sample estimates:

mean of x mean of y

8.733333 4.133333

\end{verbatim}
\end{framed}

This version of the test does not assume that the variance of the two samples is equal and performs a Welch two sample t-test. The "classic" version of the t-test can be run as follows:

\begin{framed}
\begin{verbatim}
> t.test(var1, var2, var.equal=T)

Two Sample t-test

data: x1 and x2

t = 4.0369, df = 28, p-value = 0.0003806

alternative hypothesis: true difference in means is not equal to 0

95 percent confidence interval:

2.265883 6.934117

sample estimates:

mean of x mean of y

8.733333 4.133333

\end{verbatim}
\end{framed}
%=====================================================%


A paired t-test is simple to run; just add \texttt{paired= TRUE} to the basic command. Now the variances of the two samples are considered equal and the basic version is performed. To run a t-test on paired data you add a new term:

\begin{framed}
\begin{verbatim}
> t.test(var1, var2, paired=T)

Paired t-test

data: x1 and x2

t = 4.3246, df = 14, p-value = 0.0006995

alternative hypothesis: true difference in means is not equal to 0

95 percent confidence interval:

2.318620 6.881380

sample estimates:

mean of the differences

4.6

\end{verbatim}
\end{framed}


\section{Part 2 More inference procedures}
\subsection{Formal tests for Normality}

\begin{itemize}
\item ad.test for outliers Anderson Darling test
\item shapiro Wilk test for normality
\item ks.test for distributions (Kolmogorov Smirnov test)
\item grubbs test for outliers
\item chi square test
\end{itemize}

useful for categorical data. Consider a two by two contingency table.


apropos test to find other tests available in R. Chi-squared Test of Independence

Two random variables x and y are called independent if the probability distribution of one variable is not affected by the presence of another.

Assume fij is the observed frequency count of events belonging to both i-th category of xand j-th category of y. Also assume eij to be the corresponding expected count if x and yare independent. The null hypothesis of the independence assumption is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level α.



Example

 Graphical procedures for assessing normality

qqplot

histograms


\subsection{Quantile-Quantile Plots}

Description

qqnorm is a generic function the default method of which produces a normal QQ plot of the values in y. qqline adds a line to a normal quantile-quantile plot which passes through the first and third quartiles.

qqplot produces a QQ plot of two datasets.

Graphical parameters may be given as arguments to qqnorm, qqplot and qqline.

%-------------------------------------------------------------------------------%


Part 3 linear model

scatterplots


\subsection{Scatter Plot:}

 A scatter plot of two variables shows the values of one variable on the Y axis and the values of the other variable on the X axis. Scatter plots are well suited for revealing the relationship between two variables. The scatter plot shown in Figure 4 illustrates data from one of Galileo's classic experiments in which he observed the distance traveled balls traveled after being dropped on a incline as a function of their release height.



A standard practice when performing a bivariate analysis is to construct a scatter plot. This is easily implementable using the R command plot.

Note that the predictor variable precedes the response variable.

More complex scatterplots, with better visual appeal, can be constructed. We will lok at this more later on in the semester.


The Pearson product-moment correlation coefficient is a measure of the strength of the linear relationship between two variables. It is referred to as Pearson's correlation or simply as the correlation coefficient. If the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables.

The symbol for Pearson's correlation is "ρ" when it is measured in the population and "r" when it is measured in a sample. Because we will be dealing almost exclusively with samples, we will use r to to represent Pearson's correlation unless otherwise noted.

Pearson's r can range from -1 to 1. An r of -1 indicates a perfect negative linear relationship between variables, an r of 0 indicates no linear relationship between variables, and an r of 1 indicates a perfect positive relationship between variables. 


%% - http://onlinestatbook.com/chapter4/intro.html

%=====================================================%
\subsection{Simple Linear Regression}


The intercept is the estimated value of the response variable when the predictor variable is set to zero. In many cases it would make sense that there would be a non zero value in this instance.

The slope is the rate at which response variable changes as the predictor cbanfes. A negative slope insicates a negative linear relationship.

%=====================================================%
\subsection{Fitting a Model}

 To fit an ordinary linear model with fertility change as the response and setting and effort as predictors, try

\begin{framed}
\begin{verbatim}
 > lmfit = lm( change ~ setting + effort )
\end{verbatim}
\end{framed}

 Note first that lm is a function, and we assign the result to an object that we choose to call lmfit (for linear model fit). This stores the results of the fit for later examination


\subsection{Correlation }

\begin{itemize}
\item Correlation describes the strength of a relationship between two numeric variables.
\item The strength of the relation is represented in a numeric value known at the correlation coefficient. This coefficient can take a value between -1 and1. Additionally there are no units.

\item A value close to 1 indicates strong linear relation. A value close to Zero indicates no linear relationship at all.importantly it is assumed that the relationship is linear. Some variables will have a non linear relationship.

\item The relevant R command is cor(). There is also a more complex command called cor.test. This command additionally provides a hypothesis test for the estimate.
\begin{itemize}
\item[Ho:] The correlation coefficient for the population of values is zero. No linear relationship.
\item[Ha:] The coefficient is not zero. Linear relationship exists.
\end{itemize}
\item A confidence interval for the coefficient is provided for in the R output. If the interval includes 0 then we fail to reject the null hypothesis.
\end{itemize}
\subsection{Anscombe Quartet}
Anscombe quartet is a famous data set used to demonstrate why a full set of statistical analyses should be used in conjunction, rather than rely on one analysis. This data set comprises four set of data.
%====================================================================================%

Correlation and Significance tests

Getting a correlation coefficient is generally only half the story; you will want to know if the relationship is significant. The cor() function in R can be extended to provide the significance testing required. The function is cor.test()


To run a correlation test we type:
\begin{framed}
\begin{veratim}
> cor.test(var1, var2, method = "method")
\end{verbatim}
\end{framed}

The default method is "pearson" so you may omit this if that is what you want. If you type "kendall" or "spearman" then you will get the appropriate significance test.

As usual with R it is a good idea to assign a variable name to your result in case you want to perfom additional operations.

Simple Linear Regression

Correlation Test

correlation

pearsons rho



 multiple linear regression


linear regression whete there is more than one predictor variable.

very simple to implement in R.


coefficient of determination. 

for sime lunear regression this is equivalent to the correlation squared


also rank correlation coefficient.

Spearman correlation




explanatory variables

predictor variables

slope intercept regression  equation

plitting both variables against each other

histograms 


underlying assumptions normal distribution of residual terms

%======================================================================================%
influence

outliers

 non linear regression

 polynomial regression

