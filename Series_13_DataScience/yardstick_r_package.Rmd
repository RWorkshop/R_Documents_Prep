---
title: "Yardstick R package"
subtitle: "Data Science With R"
author: DragonflyStats.github.io
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

The {yardstick} R package
===================================

yardstick: Tidy Characterizations of Model Performance
Tidy tools for quantifying how well model fits to a data set such as confusion matrices, class probability curve summaries, and regression metrics (e.g., RMSE).


---


```{r include= FALSE,echo=FALSE,warning=FALSE}
library(yardstick)

library(janitor)
library(tidyverse)
library(magrittr)

```


Performance Measures

Accuracy
Precision
Recall
The F-Maasure


%http://www.kdnuggets.com/faq/precision-recall.html




Calculating precision and recall is actually quite easy. Imagine there are 100 positive cases among 10,000 cases. You want to predict which ones are positive, and you pick 200 to have a better chance of catching many of the 100 positive cases.  You record the IDs of your predictions, and when you get the actual results you sum up how many times you were right or wrong. There are four ways of being right or wrong:


* TN / True Negative: case was negative and predicted negative
* TP / True Positive: case was positive and predicted positive
* FN / False Negative: case was positive but predicted negative
* FP / False Positive: case was negative but predicted positive


Makes sense so far? Now you count how many of the 10,000 cases fall in each bucket, say:

%\begin{array}{|c|c|c|}
%&Predicted Negative & Predicted Positive\\
%Negative Cases & TN: 9,760 & FP: 140 \\
%Positive Cases & FN: 40 & TP: 60 \\
%\end{array}

%------------------------------------------------------------------------------------------%

Now, your boss asks you three questions:

What percent of your predictions were correct? 

You answer: the ``accuracy" was (9,760+60) out of 10,000 = 98.2\%

What percent of the positive cases did you catch? 

You answer: the "recall" was 60 out of 100 = 60\%

What percent of positive predictions were correct? 

You answer: the "precision" was 60 out of 200 = 30\%

%------------------------------------------------------%




<pre><code>
library(yardstick)

library(janitor)
library(tidyverse)
library(magrittr)
</code></pre>

```{r}
head(two_class_example)
```

---

### Cross-Tabulation

```{r}

two_class_example %>%tabyl(truth)

```

```{r}

two_class_example %>%tabyl(predicted)

```

```{r}

two_class_example %>% tabyl(truth,predicted) 

```

---

### Cross-Tabulation

```{r}

two_class_example %>% 
  tabyl(truth,predicted) %>%
  adorn_totals("both")

```


```{r}

myData <- two_class_example %>% sample_frac(0.80)

myData %>% 
  tabyl(truth,predicted) %>%
  adorn_totals("both")
```

---

### Accuracy Precision and Recall

```{r}



accuracy(myData , truth, predicted)

```


#### Precision

```{r}



precision(myData , truth, predicted)

```



#### Recall 

```{r}


recall(myData , truth, predicted)

```

---



```{r}



ppv(myData , truth, predicted)

```



